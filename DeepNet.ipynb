{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "import random\n",
    "print(os.listdir(\"../input\"))\n",
    "print(os.listdir())\n",
    "from tqdm import tqdm\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.densenet import DenseNet201, preprocess_input\n",
    "from keras.callbacks import TensorBoard, LearningRateScheduler, ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.regularizers import l2\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load seedling species and extract file names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "CATEGORIES = os.listdir(\"../input/train/\")\n",
    "CATEGORIES.sort()\n",
    "\n",
    "train = []\n",
    "for category_id, category in enumerate(CATEGORIES):\n",
    "    for file in os.listdir(os.path.join('../input/train/', category)):\n",
    "        train.append(['../input/train/{}/{}'.format(category, file), category_id, category])\n",
    "train = pd.DataFrame(train, columns=['file', 'category_id', 'category'])\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5dd38ef423b9e9b803d422185418e4e5308f3ee7"
   },
   "source": [
    "Split training and validation data 80/20 stratified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8d4faad9b8cc8c03125b2bda60c5e9f18b35fcc1"
   },
   "outputs": [],
   "source": [
    "trainSample = pd.concat([train[train['category'] == c].sample(frac=0.8) for c in CATEGORIES])\n",
    "validate = train[~train.file.isin(trainSample['file'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "67a5b8bc1143953dc46c440dd04761f51220d411"
   },
   "source": [
    "Load training and validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a46f576223f9b19e1a5030c465f40bf91a8188f3"
   },
   "outputs": [],
   "source": [
    "def read_img(filepath, size):\n",
    "    img = image.load_img(os.path.join(filepath), target_size=size)\n",
    "    img = image.img_to_array(img)\n",
    "    return img\n",
    "\n",
    "INPUT_SIZE = 224\n",
    "trainX = np.zeros((len(trainSample), INPUT_SIZE, INPUT_SIZE, 3), dtype='float32')\n",
    "for i, file in tqdm(enumerate(trainSample['file'])):\n",
    "    img = read_img(file, (INPUT_SIZE, INPUT_SIZE))\n",
    "    trainX[i] = img\n",
    "    \n",
    "valX =  np.zeros((len(validate), INPUT_SIZE, INPUT_SIZE, 3), dtype='float32')\n",
    "for i, file in tqdm(enumerate(validate['file'])):\n",
    "    img = read_img(file, (INPUT_SIZE, INPUT_SIZE))\n",
    "    valX[i] = img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One hot endcoding for response variables. Needed for categorical crossentroy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "33d4e74dd2cd5e7d618485090d45ffce1ab18db3"
   },
   "outputs": [],
   "source": [
    "ohc = OneHotEncoder(sparse=False)\n",
    "ohc.fit(trainSample[['category']])\n",
    "trainY = ohc.transform(trainSample[['category']])\n",
    "valY = ohc.transform(validate[['category']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preprocessing and augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c8a5836dc2d9965bb37277603d8928e85594710d"
   },
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=90,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    #shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    preprocessing_function=preprocess_input)\n",
    "\n",
    "val_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load DeepNet. Start by training only the top layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7f72f1c3ca744ef4399abc948b6a8f2e3e6a1dae"
   },
   "outputs": [],
   "source": [
    "basic_model = DenseNet201(include_top=False, weights='imagenet', pooling='avg')\n",
    "\n",
    "for layer in basic_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "input_tensor = basic_model.input\n",
    "# build top\n",
    "x = basic_model.output\n",
    "x = Dropout(.5)(x)\n",
    "x = Dense(len(CATEGORIES), activation='softmax')(x)\n",
    "\n",
    "best_model_file = 'DenseNet201-224x224.h5'\n",
    "model = Model(inputs=input_tensor, outputs=x)\n",
    "model.compile(optimizer=RMSprop(1e-3), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add Callbacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e08893526e6712a3ffb2fad027247ca57a18fe0a"
   },
   "outputs": [],
   "source": [
    "callbacks = [EarlyStopping(monitor='val_loss', patience=5, verbose=1, min_delta=1e-5),\n",
    "             ModelCheckpoint(filepath=best_model_file, verbose=1,\n",
    "                             save_best_only=True, save_weights_only=True, mode='auto')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "799cacc397c68d4510baaf00d56906499c90834a"
   },
   "source": [
    "Train Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d7ab334898022a9e25c0676a26bc050ec14f8cf7"
   },
   "outputs": [],
   "source": [
    "model.fit_generator(train_datagen.flow(trainX, trainY, batch_size=16), epochs=40, \n",
    "                    validation_data=val_datagen.flow(valX, valY, batch_size=16),\n",
    "                    callbacks=callbacks,\n",
    "                    steps_per_epoch = trainSample.shape[0]/16,\n",
    "                    validation_steps = validate.shape[0]/16,\n",
    "                    #workers=4,\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train on all layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "06fc041f0abd95e56375085551b88bfc43527cda"
   },
   "outputs": [],
   "source": [
    "model.load_weights(best_model_file)\n",
    "for layer in model.layers:\n",
    "    layer.W_regularizer = l2(1e-2)\n",
    "    layer.trainable = True\n",
    "\n",
    "model.compile(optimizer=RMSprop(1e-4), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New Callbacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "898697fda560148e7bc23ece003bc576bee6397a"
   },
   "outputs": [],
   "source": [
    "callbacks = [EarlyStopping(monitor='val_loss', patience=10, verbose=1, min_delta=1e-5),\n",
    "             ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, cooldown=1, \n",
    "                               verbose=1, min_lr=1e-7),\n",
    "             ModelCheckpoint(filepath=best_model_file, verbose=1,\n",
    "                             save_best_only=True, save_weights_only=True, mode='auto')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c1a86db171355bf6a90ef1209da3c40ec89c8ec3",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit_generator(train_datagen.flow(trainX, trainY, batch_size=16), epochs=100, \n",
    "                    validation_data=val_datagen.flow(valX, valY, batch_size=16),\n",
    "                    callbacks=callbacks,\n",
    "                    steps_per_epoch = trainSample.shape[0]/16,\n",
    "                    validation_steps = validate.shape[0]/16,\n",
    "                    #workers=4,\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Test Data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3c7729aaf57797154e3c7a1dbd84a89f88e7bc36"
   },
   "outputs": [],
   "source": [
    "test = []\n",
    "for file in os.listdir(\"../input/test\"):\n",
    "    test.append(['../input/test/{}'.format(file), file])\n",
    "test = pd.DataFrame(test, columns=['filepath', 'file'])\n",
    "\n",
    "testX = np.zeros((len(test), INPUT_SIZE, INPUT_SIZE, 3), dtype='float32')\n",
    "for i, filepath in tqdm(enumerate(test['filepath'])):\n",
    "    img = read_img(filepath, (INPUT_SIZE, INPUT_SIZE))\n",
    "    testX[i] = img\n",
    "print('test Images shape: {} size: {:,}'.format(testX.shape, testX.size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save predictions, both results and probability scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c3d668d35d9b30c0f4eca547ebf71270cf5cd955"
   },
   "outputs": [],
   "source": [
    "predictions = model.predict_generator(val_datagen.flow(testX, shuffle=False,\n",
    "                                                       batch_size=1),\n",
    "                                     steps=testX.shape[0])\n",
    "\n",
    "preds = []\n",
    "for i in range(len(predictions)):\n",
    "    pos = np.argmax(predictions[i])\n",
    "    preds.append(CATEGORIES[pos])\n",
    "    \n",
    "pred_df = pd.DataFrame({'file': test['filepath'].apply(lambda x: x.split('/')[3]), 'species': preds})\n",
    "pred_df.to_csv('pred.csv', index=False)\n",
    "predictions_df = pd.DataFrame(predictions)\n",
    "predictions_df['file'] = test['filepath'].apply(lambda x: x.split('/')[3])\n",
    "predictions_df.to_csv('predictions.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
